{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_validate_lengths'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-def590a13184>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mskimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mzipfile\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mZipFile\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0m_raise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/util/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mapply_parallel\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapply_parallel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marraycrop\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcrop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_regular_grid\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mregular_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregular_seeds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0munique_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/skimage/util/arraycrop.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marraypad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_validate_lengths\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name '_validate_lengths'"
     ]
    }
   ],
   "source": [
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import datetime\n",
    "from torch.utils.data import Dataset\n",
    "from skimage.transform import resize\n",
    "\n",
    "from zipfile import ZipFile\n",
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from Dataset import FaceDataset,Scale,ToTensor\n",
    "from Models import resblock,network_9layers,network_29layers,network_29layers_v2,save_model\n",
    "from losses import batch_hard_triplet_loss\n",
    "import skimage\n",
    "from skimage.io import imread,imsave\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm \n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Dec 27 14:58:32 2019       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.39       Driver Version: 418.39       CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  On   | 00000000:B2:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    43W / 300W |      0MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6.7\n"
     ]
    }
   ],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('image_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light CNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msk = np.random.rand(len(df)) < 0.8\n",
    "\n",
    "df_train=df[msk]\n",
    "df_test=df[~msk]\n",
    "print(len(df),'\\n',len(df_train),'\\n',len(df_test))\n",
    "df_train.to_csv('train.csv')\n",
    "df_test.to_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('train/')\n",
    "os.mkdir('test/')\n",
    "for label in tqdm(df['Label'].unique()):\n",
    "    os.mkdir('train/'+str(label))\n",
    "    os.mkdir('test/'+str(label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25325\n",
      "25325\n"
     ]
    }
   ],
   "source": [
    "transform=torchvision.transforms.Compose([Scale(),ToTensor()])\n",
    "\n",
    "train_dataset=FaceDataset('img_align_celeba','train.csv',transform)\n",
    "train_loader=DataLoader(train_dataset,batch_size=8,shuffle=True)\n",
    "test_dataset=FaceDataset('img_align_celeba','test.csv',transform)\n",
    "test_loader=DataLoader(test_dataset,batch_size=8,shuffle=True)\n",
    "print(test_loader.__len__())\n",
    "print(train_loader.__len__())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_loader(loader):\n",
    "    for i,sample in enumerate(loader):\n",
    "        #print(sample['image'].shape)\n",
    "        if i==1:\n",
    "            anchor_image, pos_image,neg_image= sample['anc']\\\n",
    "            , sample['pos'],sample['neg']\n",
    "            \n",
    "            print(anchor_image.shape)\n",
    "            \n",
    "#             anchor_image=np.squeeze(anchor_image.transpose(1,2,0),axis=2)\n",
    "#             pos_image=np.squeeze(pos_image.transpose(1,2,0),axis=2)\n",
    "#             neg_image=np.squeeze(neg_image.transpose(1,2,0),axis=2)\n",
    "            \n",
    "            \n",
    "#             #print(image)\n",
    "#             fig=plt.figure()\n",
    "#             plt.imshow(anchor_image)\n",
    "#             fig1=plt.figure()\n",
    "#             plt.imshow(pos_image)\n",
    "#             fig2=plt.figure()\n",
    "#             plt.imshow(neg_image)\n",
    "           \n",
    "            break\n",
    "visualize_loader(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "block=resblock\n",
    "model=network_29layers_v2(block, [1, 2, 3, 4])\n",
    "\n",
    "# model_start_date=datetime.datetime.now().strftime(\"%Y_%m_%d\")\n",
    "# BEST_MODEL_PATH=os.path.join(os.getcwd(),'model_{}'.format(model_start_date))\n",
    "# if not os.path.exists(BEST_MODEL_PATH):\n",
    "#     os.mkdir(BEST_MODEL_PATH)\n",
    "#     print('model_{} dir has been made'.format(model_start_date))\n",
    "# print(\"Model's state_dict:\")\n",
    "# writer = SummaryWriter('model_{}/dapi_seg_experiment_{}'.format(model_start_date,1))\n",
    "# for param_tensor in model.state_dict():\n",
    "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
    "\n",
    "\n",
    "# writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(torch.ones((8,1,128,128))).shape)\n",
    "import tensorflow as tf\n",
    "a=tf.ones((8,8))\n",
    "b=tf.linalg.diag_part(a)\n",
    "print(b.shape,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b1=tf.expand_dims(b, 0)\n",
    "print(b1.shape)\n",
    "b2=tf.expand_dims(b, 1)\n",
    "print(b2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "optimizer_selected='adam'\n",
    "batchsize=4\n",
    "no_steps=train_dataset.__len__()//batchsize\n",
    "restart_epochs=8\n",
    "num_epochs=10\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "criterion=TripletLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "\n",
    "if optimizer_selected=='adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(),lr=10e-03, betas=(0.9, 0.98),weight_decay=0.02)\n",
    "else:\n",
    "    optimizer = torch.optim.SGD(model.parameters(),lr=10e-03, momentum=0.8,nesterov=True)\n",
    "\n",
    "scheduler=torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, restart_epochs*no_steps,\\\n",
    "                                                     eta_min=10e-012, last_epoch=-1)\n",
    "\n",
    "best_val=10e06\n",
    "for epoch in range(num_epochs):\n",
    "    loop=tqdm(train_loader)\n",
    "    print(\"Learning Rate : {}\".format(optimizer.state_dict()['param_groups'][-1]['lr']))\n",
    "    # loop over the dataset multiple times\n",
    "    \n",
    "\n",
    "    for mode in ['train','eval']:\n",
    "     \n",
    "        if mode == 'train':\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            model.train()\n",
    "\n",
    "            for i, sample_batched in (enumerate(loop)):\n",
    "                loop.set_description('Epoch {}/{}'.format(epoch + 1, num_epochs))\n",
    "                #print(\"epoch {},iter {}\".format(epoch,i))\n",
    "\n",
    "                # zero the parameter gradients\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                anc_batch, pos_batch,neg_batch = sample_batched['anc'],sample_batched['pos'],sample_batched['neg']\n",
    "                \n",
    "                anc_batch, pos_batch,neg_batch = anc_batch.to(device, dtype = torch.float)\\\n",
    "                ,pos_batch.to(device, dtype = torch.float),neg_batch.to(device,dtype=torch.float)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                anc_embeddings=model(anc_batch)\n",
    "                pos_embeddings=model(pos_batch)\n",
    "                neg_embeddings=model(neg_batch)\n",
    "                \n",
    "                #outputs = torch.sigmoid()\n",
    "                \n",
    "                loss = criterion(anc_embeddings,pos_embeddings,neg_embeddings)\n",
    "        \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                scheduler.step()\n",
    "\n",
    "                    \n",
    "                loop.set_postfix(train_loss=loss.detach().item())\n",
    "                \n",
    "                 \n",
    "                    \n",
    "        elif mode =='eval':\n",
    "            optimizer.zero_grad()\n",
    "            samples_test=len(test_loader)\n",
    "            model.eval()\n",
    "            final_test_loss=0\n",
    "            \n",
    "            for j, test_sample in enumerate(test_loader):\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                # get the inputs; data is a list of [inputs, labels]\n",
    "                anc_batch_test, pos_batch_test,neg_batch_test = test_sample['anc'],test_sample['pos'],test_sample['neg']\n",
    "                \n",
    "                anc_batch_test, pos_batch_test,neg_batch_test = anc_batch_test.to(device, dtype = torch.float)\\\n",
    "                ,pos_batch_test.to(device, dtype = torch.float),neg_batch_test.to(device,dtype=torch.float)\n",
    "\n",
    "                # forward + backward + optimize\n",
    "                anc_embeddings_test=model(anc_batch_test)\n",
    "                pos_embeddings_test=model(pos_batch_test)\n",
    "                neg_embeddings_test=model(neg_batch_test)\n",
    "                \n",
    "                #outputs = torch.sigmoid()\n",
    "                \n",
    "                test_loss = criterion(anc_embeddings_test,pos_embeddings_test,neg_embeddings_test)\n",
    "        \n",
    "                \n",
    "                final_test_loss+=test_loss.detach().item()\n",
    "                \n",
    "                if j%100==99:\n",
    "                    writer.add_scalar('testing loss',test_loss.detach().item(),epoch * len(test_loader) + j)\n",
    "                \n",
    "            print(\"final test_loss: {}\".format(final_test_loss/(j+1)))\n",
    "            \n",
    "            if final_test_loss/(j+1)<best_val:\n",
    "                best_val=final_test_loss/(j+1)\n",
    "                name=BEST_MODEL_PATH+'/model_optim.pth'\n",
    "                save_model(model,optimizer,name,scheduler=None)\n",
    "                print(\"saved model with test dice score: {}\".format(best_val))\n",
    "\n",
    "name=BEST_MODEL_PATH+'/model_final.pth'\n",
    "save_model(model,optimizer,name,scheduler=None)\n",
    "        \n",
    "        \n",
    "\n",
    "print('Finished Training')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z=b1-2*a+b2\n",
    "print(z.shape,z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class conv_block(nn.Module):\n",
    "    def __init__(self,channel_in,channel_out,kernel_size,padding,stride):\n",
    "        super().__init__()\n",
    "        self.conv=nn.Sequential(nn.Conv2d(channel_in,channel_out,kernel_size=kernel_size,stride=stride,padding=padding,bias=True),\n",
    "                                nn.BatchNorm2d(channel_out),\n",
    "                                nn.ReLU(inplace=True))\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x=self.conv(x)\n",
    "        return x\n",
    "\n",
    "# class de_conv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
